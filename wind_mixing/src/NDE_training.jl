function predict_NDE(NN, x, top, bottom)
    interior = NN(x)
    return [top; interior; bottom]
end

function predict_NDE_convective_adjustment(NN, x, top, bottom, D_face, D_cell, Îº, Nz)
    interior = NN(x)
    T = @view x[2Nz + 1:3Nz]
    wT = [top; interior; bottom]
    âˆ‚Tâˆ‚z = D_face * T
    âˆ‚z_Îºâˆ‚Tâˆ‚z = D_cell * min.(0f0, Îº .* âˆ‚Tâˆ‚z)
    return - D_cell * wT .+ âˆ‚z_Îºâˆ‚Tâˆ‚z
end

function prepare_time_window(t, trange)
    return Float32.(t[trange])
end

function prepare_training_data(uvT, trange)
    return Float32.(uvT[:,trange])
end

function save_NDE_weights(weights, size_uw_NN, size_vw_NN, size_wT_NN, FILE_PATH=pwd(), filename="weights")
    uw_weights = weights[1:size_uw_NN]
    vw_weights = weights[size_uw_NN + 1:size_uw_NN + size_vw_NN]
    wT_weights = weights[size_uw_NN + size_vw_NN + 1:size_uw_NN + size_vw_NN + size_wT_NN]
    uw_NN_params = Dict(:weights => uw_weights)
    bson(joinpath(FILE_PATH, "uw_$filename.bson"), uw_NN_params)

    vw_NN_params = Dict(:weights => vw_weights)
    bson(joinpath(FILE_PATH, "vw_$filename.bson"), vw_NN_params)

    wT_NN_params = Dict(:weights => wT_weights)
    bson(joinpath(FILE_PATH, "wT_$filename.bson"), wT_NN_params)
end

function cb(args...)
    @info "loss = $(args[2])"
    false
end

function prepare_parameters_NDE_training(ğ’Ÿtrain, uw_NN, vw_NN, wT_NN, f=1f-4, Nz=32)
    H = Float32(abs(ğ’Ÿtrain.uw.z[end] - ğ’Ÿtrain.uw.z[1]))
    Ï„ = Float32(abs(ğ’Ÿtrain.t[:,1][end] - ğ’Ÿtrain.t[:,1][1]))
    u_scaling = ğ’Ÿtrain.scalings["u"]
    v_scaling = ğ’Ÿtrain.scalings["v"]
    T_scaling = ğ’Ÿtrain.scalings["T"]
    uw_scaling = ğ’Ÿtrain.scalings["uw"]
    vw_scaling = ğ’Ÿtrain.scalings["vw"]
    wT_scaling = ğ’Ÿtrain.scalings["wT"]
    Î¼_u = Float32(u_scaling.Î¼)
    Î¼_v = Float32(v_scaling.Î¼)
    Ïƒ_u = Float32(u_scaling.Ïƒ)
    Ïƒ_v = Float32(v_scaling.Ïƒ)
    Ïƒ_T = Float32(T_scaling.Ïƒ)
    Ïƒ_uw = Float32(uw_scaling.Ïƒ)
    Ïƒ_vw = Float32(vw_scaling.Ïƒ)
    Ïƒ_wT = Float32(wT_scaling.Ïƒ)
    uw_weights, re_uw = Flux.destructure(uw_NN)
    vw_weights, re_vw = Flux.destructure(vw_NN)
    wT_weights, re_wT = Flux.destructure(wT_NN)
    weights = Float32[uw_weights; vw_weights; wT_weights]
    D_cell = Float32.(Dá¶œ(Nz, 1 / Nz))
    D_face = Float32.(Dá¶ (Nz, 1 / Nz))
    size_uw_NN = length(uw_weights)
    size_vw_NN = length(vw_weights)
    size_wT_NN = length(wT_weights)
    uw_range = 1:size_uw_NN
    vw_range = size_uw_NN + 1:size_uw_NN + size_vw_NN
    wT_range = size_uw_NN + size_vw_NN + 1:size_uw_NN + size_vw_NN + size_wT_NN
    return f, H, Ï„, Nz, u_scaling, T_scaling, uw_scaling, vw_scaling, wT_scaling, Î¼_u, Î¼_v, Ïƒ_u, Ïƒ_v, Ïƒ_T, Ïƒ_uw, Ïƒ_vw, Ïƒ_wT, weights, re_uw, re_vw, re_wT, D_cell, D_face, size_uw_NN, size_vw_NN, size_wT_NN, uw_range, vw_range, wT_range
end

function train_NDE(uw_NN, vw_NN, wT_NN, ğ’Ÿtrain, tsteps, timestepper, optimizers, epochs, FILE_PATH, stage, n_simulations, maxiters=500)
    f, H, Ï„, Nz, u_scaling, T_scaling, uw_scaling, vw_scaling, wT_scaling, Î¼_u, Î¼_v, Ïƒ_u, Ïƒ_v, Ïƒ_T, Ïƒ_uw, Ïƒ_vw, Ïƒ_wT, weights, re_uw, re_vw, re_wT, D_cell, D_face, size_uw_NN, size_vw_NN, size_wT_NN, uw_range, vw_range, wT_range = prepare_parameters_NDE_training(ğ’Ÿtrain, uw_NN, vw_NN, wT_NN)

    n_steps = Int(length(@view(ğ’Ÿtrain.t[:,1])) / n_simulations)

    Îº = 10f0

    function NDE!(dx, x, p, t)
        uw_weights = p[uw_range]
        vw_weights = p[vw_range]
        wT_weights = p[wT_range]
        uw_top, uw_bottom, vw_top, vw_bottom, wT_top, wT_bottom = p[wT_range[end] + 1:end]
        uw_NN = re_uw(uw_weights)
        vw_NN = re_vw(vw_weights)
        wT_NN = re_wT(wT_weights)
        A = - Ï„ / H
        B = f * Ï„
        u = x[1:Nz]
        v = x[Nz + 1:2Nz]
        T = x[2Nz + 1:3Nz]
        dx[1:Nz] .= A .* Ïƒ_uw ./ Ïƒ_u .* D_cell * predict_NDE(uw_NN, x, uw_top, uw_bottom) .+ B ./ Ïƒ_u .* (Ïƒ_v .* v .+ Î¼_v) # nondimensional gradient
        dx[Nz + 1:2Nz] .= A .* Ïƒ_vw ./ Ïƒ_v .* D_cell * predict_NDE(vw_NN, x, vw_top, vw_bottom) .- B ./ Ïƒ_v .* (Ïƒ_u .* u .+ Î¼_u)
        dx[2Nz + 1:3Nz] .= A .* Ïƒ_wT ./ Ïƒ_T .* predict_NDE(wT_NN, x, wT_top, wT_bottom)
    end

    uvTâ‚€s = [Float32.(ğ’Ÿtrain.uvT_scaled[:,n_steps * i + tsteps[1]]) for i in 0:n_simulations - 1]
    t_train = prepare_time_window(ğ’Ÿtrain.t[:,1], tsteps)
    uvT_trains = [prepare_training_data(ğ’Ÿtrain.uvT_scaled[:,n_steps * i + 1:n_steps * (i + 1)], tsteps) for i in 0:n_simulations - 1]
    t_train = Float32.(t_train ./ Ï„)
    tspan_train = (t_train[1], t_train[end])
    BCs = [[Float32.(ğ’Ÿtrain.uw.scaled[1,n_steps * i + tsteps[1]]),
            Float32.(ğ’Ÿtrain.uw.scaled[end,n_steps * i + tsteps[1]]),
            Float32.(ğ’Ÿtrain.vw.scaled[1,n_steps * i + tsteps[1]]),
            Float32.(ğ’Ÿtrain.vw.scaled[end,n_steps * i + tsteps[1]]),
            Float32.(ğ’Ÿtrain.wT.scaled[1,n_steps * i + tsteps[1]]),
            Float32.(ğ’Ÿtrain.wT.scaled[end,n_steps * i + tsteps[1]])] for i in 0:n_simulations - 1]

    prob_NDEs = [ODEProblem(NDE!, uvTâ‚€s[i], tspan_train) for i in 1:n_simulations]

    function loss(weights, BCs)
        sols = [Float32.(Array(solve(prob_NDEs[i], timestepper, p=[weights; BCs[i]], reltol=1f-3, sensealg=InterpolatingAdjoint(), saveat=t_train))) for i in 1:n_simulations]
        return mean(Flux.mse.(sols, uvT_trains))
    end

    f_loss = OptimizationFunction(loss, GalacticOptim.AutoZygote())
    prob_loss = OptimizationProblem(f_loss, weights, BCs)

    for i in 1:length(optimizers), epoch in 1:epochs
        iter = 1
        opt = optimizers[i]
        function cb(args...)
            if iter <= maxiters
                @info "NDE, loss = $(args[2]), stage $stage, optimizer $i/$(length(optimizers)), epoch $epoch/$epochs, iteration = $iter/$maxiters"
                write_data_NDE_training(FILE_PATH, args[2], re_uw(args[1][uw_range]), re_vw(args[1][vw_range]), re_wT(args[1][wT_range]), stage)
            end
            iter += 1
            false
        end
        res = solve(prob_loss, opt, cb=cb, maxiters=maxiters)
        weights .= res.minimizer
    end
    return re_uw(weights[uw_range]), re_vw(weights[vw_range]), re_wT(weights[wT_range])
end

function train_NDE_convective_adjustment(uw_NN, vw_NN, wT_NN, ğ’Ÿtrain, tsteps, timestepper, optimizers, epochs, FILE_PATH, stage, n_simulations, maxiters=500)
    f, H, Ï„, Nz, u_scaling, T_scaling, uw_scaling, vw_scaling, wT_scaling, Î¼_u, Î¼_v, Ïƒ_u, Ïƒ_v, Ïƒ_T, Ïƒ_uw, Ïƒ_vw, Ïƒ_wT, weights, re_uw, re_vw, re_wT, D_cell, D_face, size_uw_NN, size_vw_NN, size_wT_NN, uw_range, vw_range, wT_range = prepare_parameters_NDE_training(ğ’Ÿtrain, uw_NN, vw_NN, wT_NN)

    n_steps = Int(length(@view(ğ’Ÿtrain.t[:,1])) / n_simulations)

    Îº = 10f0

    function NDE!(dx, x, p, t)
        uw_weights = p[uw_range]
        vw_weights = p[vw_range]
        wT_weights = p[wT_range]
        uw_top, uw_bottom, vw_top, vw_bottom, wT_top, wT_bottom = p[wT_range[end] + 1:end]
        uw_NN = re_uw(uw_weights)
        vw_NN = re_vw(vw_weights)
        wT_NN = re_wT(wT_weights)
        A = - Ï„ / H
        B = f * Ï„
        u = x[1:Nz]
        v = x[Nz + 1:2Nz]
        T = x[2Nz + 1:3Nz]
        dx[1:Nz] .= A .* Ïƒ_uw ./ Ïƒ_u .* D_cell * predict_NDE(uw_NN, x, uw_top, uw_bottom) .+ B ./ Ïƒ_u .* (Ïƒ_v .* v .+ Î¼_v) # nondimensional gradient
        dx[Nz + 1:2Nz] .= A .* Ïƒ_vw ./ Ïƒ_v .* D_cell * predict_NDE(vw_NN, x, vw_top, vw_bottom) .- B ./ Ïƒ_v .* (Ïƒ_u .* u .+ Î¼_u)
        dx[2Nz + 1:3Nz] .= -A .* Ïƒ_wT ./ Ïƒ_T .* predict_NDE_convective_adjustment(wT_NN, x, wT_top, wT_bottom, D_face, D_cell, Îº, Nz)
    end

    uvTâ‚€s = [Float32.(ğ’Ÿtrain.uvT_scaled[:,n_steps * i + tsteps[1]]) for i in 0:n_simulations - 1]
    t_train = prepare_time_window(ğ’Ÿtrain.t[:,1], tsteps)
    uvT_trains = [prepare_training_data(ğ’Ÿtrain.uvT_scaled[:,n_steps * i + 1:n_steps * (i + 1)], tsteps) for i in 0:n_simulations - 1]
    t_train = Float32.(t_train ./ Ï„)
    tspan_train = (t_train[1], t_train[end])
    BCs = [[Float32.(ğ’Ÿtrain.uw.scaled[1,n_steps * i + tsteps[1]]),
            Float32.(ğ’Ÿtrain.uw.scaled[end,n_steps * i + tsteps[1]]),
            Float32.(ğ’Ÿtrain.vw.scaled[1,n_steps * i + tsteps[1]]),
            Float32.(ğ’Ÿtrain.vw.scaled[end,n_steps * i + tsteps[1]]),
            Float32.(ğ’Ÿtrain.wT.scaled[1,n_steps * i + tsteps[1]]),
            Float32.(ğ’Ÿtrain.wT.scaled[end,n_steps * i + tsteps[1]])] for i in 0:n_simulations - 1]

    prob_NDEs = [ODEProblem(NDE!, uvTâ‚€s[i], tspan_train) for i in 1:n_simulations]

    function loss(weights, BCs)
        sols = [Float32.(Array(solve(prob_NDEs[i], timestepper, p=[weights; BCs[i]], reltol=1f-3, sensealg=InterpolatingAdjoint(), saveat=t_train))) for i in 1:n_simulations]
        return mean(Flux.mse.(sols, uvT_trains))
    end

    f_loss = OptimizationFunction(loss, GalacticOptim.AutoZygote())
    prob_loss = OptimizationProblem(f_loss, weights, BCs)

    for i in 1:length(optimizers), epoch in 1:epochs
        iter = 1
        opt = optimizers[i]
        function cb(args...)
            if iter <= maxiters
                @info "loss = $(args[2]), stage $stage, optimizer $i/$(length(optimizers)), epoch $epoch/$epochs, iteration = $iter/$maxiters"
                write_data_NDE_training(FILE_PATH, args[2], re_uw(args[1][uw_range]), re_vw(args[1][vw_range]), re_wT(args[1][wT_range]), stage)
            end
            iter += 1
            false
        end
        res = solve(prob_loss, opt, cb=cb, maxiters=maxiters)
        weights .= res.minimizer
    end
    return re_uw(weights[uw_range]), re_vw(weights[vw_range]), re_wT(weights[wT_range])
end