function DE(x, p, t, derivatives, scalings, constants, BCs)
    Î½â‚€, Î½â‚‹, Î”Ri, Riá¶œ, Pr = p

    Nz, H, Ï„, f, g, Î±  = constants.Nz, constants.H, constants.Ï„, constants.f, constants.g, constants.Î±
    Ïƒ_uw, Ïƒ_vw, Ïƒ_wT = scalings.uw.Ïƒ, scalings.vw.Ïƒ, scalings.wT.Ïƒ
    Î¼_u, Î¼_v, Ïƒ_u, Ïƒ_v, Ïƒ_T = scalings.u.Î¼, scalings.v.Î¼, scalings.u.Ïƒ, scalings.v.Ïƒ, scalings.T.Ïƒ

    D_cell = derivatives.cell
    D_face = derivatives.face

    u = @view x[1:Nz]
    v = @view x[Nz + 1:2Nz]
    T = @view x[2Nz + 1:3Nz]

    Ïµ = 1f-7
    âˆ‚uâˆ‚z = D_face * u
    âˆ‚vâˆ‚z = D_face * v
    âˆ‚Tâˆ‚z = D_face * T
    Ri = local_richardson.(âˆ‚uâˆ‚z .+ Ïµ, âˆ‚vâˆ‚z .+ Ïµ, âˆ‚Tâˆ‚z .+ Ïµ, H, g, Î±, Ïƒ_u, Ïƒ_v, Ïƒ_T)

    Î½ = Î½â‚€ .+ Î½â‚‹ .* tanh_step.((Ri .- Riá¶œ) ./ Î”Ri)
    # Î½ = zeros(Float32, 33)

    Î½âˆ‚uâˆ‚z = [-(BCs.uw.bottom - scalings.uw(0f0)); Ïƒ_u / Ïƒ_uw / H .* Î½[2:end-1] .* âˆ‚uâˆ‚z[2:end-1]; -(BCs.uw.top - scalings.uw(0f0))]
    Î½âˆ‚vâˆ‚z = [-(BCs.vw.bottom - scalings.vw(0f0)); Ïƒ_v / Ïƒ_vw / H .* Î½[2:end-1] .* âˆ‚vâˆ‚z[2:end-1]; -(BCs.vw.top - scalings.vw(0f0))]
    Î½âˆ‚Tâˆ‚z = [-(BCs.wT.bottom - scalings.wT(0f0)); Ïƒ_T / Ïƒ_wT / H .* Î½[2:end-1] ./ Pr .* âˆ‚Tâˆ‚z[2:end-1]; -(BCs.wT.top - scalings.wT(0f0))]
    
    âˆ‚uâˆ‚t = Ï„ / H * Ïƒ_uw / Ïƒ_u .* derivatives.cell * Î½âˆ‚uâˆ‚z .+ f * Ï„ / Ïƒ_u .* (Ïƒ_v .* v .+ Î¼_v)
    âˆ‚vâˆ‚t = Ï„ / H * Ïƒ_vw / Ïƒ_v .* derivatives.cell * Î½âˆ‚vâˆ‚z .- f * Ï„ / Ïƒ_v .* (Ïƒ_u .* u .+ Î¼_u)
    âˆ‚Tâˆ‚t = Ï„ / H * Ïƒ_wT / Ïƒ_T .* derivatives.cell * Î½âˆ‚Tâˆ‚z

    return [âˆ‚uâˆ‚t; âˆ‚vâˆ‚t; âˆ‚Tâˆ‚t]
end

function optimise_modified_pacanowski_philander(train_files, tsteps, timestepper, optimizers, maxiters, FILE_PATH;
                                                n_simulations, Î½â‚€ = 1f-5, Î½â‚‹ = 1f-1, Î”Ri=0.1f0, Riá¶œ=0.25f0, Pr=1f0,
                                                train_gradient=true, gradient_scaling=5f-3,
                                                training_fractions=nothing)
    @info "Loading training data"
    ð’Ÿ = WindMixing.data(train_files, scale_type=ZeroMeanUnitVarianceScaling, enforce_surface_fluxes=false)
    
    @info "Preparing constants and parameters"

    Î½â‚€_scaling = 1 / Î½â‚€
    Î½â‚‹_scaling = 1 / Î½â‚‹
    Î”Ri_scaling = 1 / Î”Ri
    Riá¶œ_scaling = 1 / Riá¶œ
    Pr_scaling = 1 / Pr

    function scale_parameter(parameter, scaling)
        return parameter * scaling
    end

    function unscale_parameter(parameter, scaling)
        return parameter / scaling
    end
    
    function prepare_parameters()
        Nz = length(ð’Ÿ.u.z)
        H = abs(ð’Ÿ.uw.z[end] - ð’Ÿ.uw.z[1])
        Ï„ = abs(ð’Ÿ.t[:,1][end] - ð’Ÿ.t[:,1][1])
        u_scaling = ð’Ÿ.scalings["u"]
        v_scaling = ð’Ÿ.scalings["v"]
        T_scaling = ð’Ÿ.scalings["T"]
        uw_scaling = ð’Ÿ.scalings["uw"]
        vw_scaling = ð’Ÿ.scalings["vw"]
        wT_scaling = ð’Ÿ.scalings["wT"]

        constants = (H=H, Ï„=Ï„, Nz=Nz, f=1f-4, Î±=2f-4, g=9.80665f0)
        scalings = (u=u_scaling, v=v_scaling, T=T_scaling, uw=uw_scaling, vw=vw_scaling, wT=wT_scaling, parameters=[Î½â‚€_scaling, Î½â‚‹_scaling, Î”Ri_scaling, Riá¶œ_scaling, Pr_scaling])
        derivatives = (cell=Float32.(Dá¶œ(Nz, 1 / Nz)), face=Float32.(Dá¶ (Nz, 1 / Nz)))
        parameters = [Î½â‚€, Î½â‚‹, Î”Ri, Riá¶œ, Pr]
        scaled_parameters = scale_parameter.(parameters, scalings.parameters)

        return constants, scalings, derivatives, scaled_parameters
    end
    
    constants, scalings, derivatives, scaled_parameters = prepare_parameters()

    Nz = constants.Nz

    D_face = derivatives.face

    n_steps = Int(length(@view(ð’Ÿ.t[:,1])) / n_simulations)

    uvTâ‚€s = [ð’Ÿ.uvT_scaled[:,n_steps * i + tsteps[1]] for i in 0:n_simulations - 1]
    t_train = ð’Ÿ.t[:,1][tsteps] ./ constants.Ï„
    tspan_train = (t_train[1], t_train[end])
    uvT_trains = [ð’Ÿ.uvT_scaled[:,n_steps * i + 1:n_steps * (i + 1)][:, tsteps] for i in 0:n_simulations - 1]

    u_trains, v_trains, T_trains = split_u.(uvT_trains, Nz), split_v.(uvT_trains, Nz), split_T.(uvT_trains, Nz)

    if train_gradient
        u_trains_gradients = [âˆ‚_âˆ‚z(sol, D_face) for sol in u_trains]
        v_trains_gradients = [âˆ‚_âˆ‚z(sol, D_face) for sol in v_trains]
        T_trains_gradients = [âˆ‚_âˆ‚z(sol, D_face) for sol in T_trains]
    end    

    @info "Setting up BCs"

    @inline function BC(i)
        index = n_steps * i + tsteps[1]
        return (uw=(bottom=ð’Ÿ.uw.scaled[1,index], top=ð’Ÿ.uw.scaled[end, index]),
                vw=(bottom=ð’Ÿ.vw.scaled[1,index], top=ð’Ÿ.vw.scaled[end, index]),
                wT=(bottom=ð’Ÿ.wT.scaled[1,index], top=ð’Ÿ.wT.scaled[end, index]))
    end

    BCs = [BC(i) for i in 0:n_simulations - 1]    

    @info "Setting up differential equations"

    prob_NDEs = [ODEProblem((x, p, t) -> DE(x, p, t, derivatives, scalings, constants, BCs[i]), uvTâ‚€s[i], tspan_train) for i in 1:n_simulations]

    function determine_loss_scalings()
        if training_fractions === nothing
            loss_scalings = (u=1, v=1, T=1, âˆ‚uâˆ‚z=gradient_scaling, âˆ‚vâˆ‚z=gradient_scaling, âˆ‚Tâˆ‚z=gradient_scaling)
        else
            unscaled_parameters = unscale_parameter.(scaled_parameters, scalings.parameters)
            sols = [Array(solve(prob_NDEs[i], timestepper, p=unscaled_parameters, reltol=1f-3, sensealg=InterpolatingAdjoint(autojacvec=ZygoteVJP()), saveat=t_train)) for i in 1:n_simulations]        
            u_sols, v_sols, T_sols = split_u.(sols, Nz), split_v.(sols, Nz), split_T.(sols, Nz)

            u_loss = mean(loss.(u_trains, u_sols))
            v_loss = mean(loss.(v_trains, v_sols))
            T_loss = mean(loss.(T_trains, T_sols))

            if train_gradient
                u_sols_gradients = [âˆ‚_âˆ‚z(sol, D_face) for sol in u_sols]
                v_sols_gradients = [âˆ‚_âˆ‚z(sol, D_face) for sol in v_sols]
                T_sols_gradients = [âˆ‚_âˆ‚z(sol, D_face) for sol in T_sols]

                âˆ‚uâˆ‚z_loss = mean(loss.(u_trains_gradients, u_sols_gradients))
                âˆ‚vâˆ‚z_loss = mean(loss.(v_trains_gradients, v_sols_gradients))
                âˆ‚Tâˆ‚z_loss = mean(loss.(T_trains_gradients, T_sols_gradients))
            else
                âˆ‚uâˆ‚z_loss = 0
                âˆ‚vâˆ‚z_loss = 0
                âˆ‚Tâˆ‚z_loss = 0
            end

            losses = (u=u_loss, v=v_loss, T=T_loss, âˆ‚uâˆ‚z=âˆ‚uâˆ‚z_loss, âˆ‚vâˆ‚z=âˆ‚vâˆ‚z_loss, âˆ‚Tâˆ‚z=âˆ‚Tâˆ‚z_loss)
            loss_scalings = calculate_loss_scalings(losses, training_fractions, train_gradient)
        end
        return loss_scalings
    end

    @info "Determining training scalings"

    loss_scalings = determine_loss_scalings()

    function loss_mpp(parameters, p)
        unscaled_parameters = unscale_parameter.(parameters, scalings.parameters)
        sols = [Array(solve(prob_NDEs[i], timestepper, p=unscaled_parameters, reltol=1f-3, sensealg=InterpolatingAdjoint(autojacvec=ZygoteVJP()), saveat=t_train)) for i in 1:n_simulations]        
        
        u_sols, v_sols, T_sols = split_u.(sols, Nz), split_v.(sols, Nz), split_T.(sols, Nz)
        u_loss = mean(loss.(u_trains, u_sols))
        v_loss = mean(loss.(v_trains, v_sols))
        T_loss = mean(loss.(T_trains, T_sols))

        losses = (u=u_loss, v=v_loss, T=T_loss, âˆ‚uâˆ‚z=0, âˆ‚vâˆ‚z=0, âˆ‚Tâˆ‚z=0)
        scaled_losses = apply_loss_scalings(losses, loss_scalings)

        return sum(scaled_losses), scaled_losses
    end

    function loss_gradient_mpp(parameters, p)
        unscaled_parameters = unscale_parameter.(parameters, scalings.parameters)
        sols = [Array(solve(prob_NDEs[i], timestepper, p=unscaled_parameters, reltol=1f-3, sensealg=InterpolatingAdjoint(autojacvec=ZygoteVJP()), saveat=t_train)) for i in 1:n_simulations]        
        
        u_sols, v_sols, T_sols = split_u.(sols, Nz), split_v.(sols, Nz), split_T.(sols, Nz)

        u_sols_gradients = [âˆ‚_âˆ‚z(sol, D_face) for sol in u_sols]
        v_sols_gradients = [âˆ‚_âˆ‚z(sol, D_face) for sol in v_sols]
        T_sols_gradients = [âˆ‚_âˆ‚z(sol, D_face) for sol in T_sols]

        u_loss = mean(loss.(u_trains, u_sols))
        v_loss = mean(loss.(v_trains, v_sols))
        T_loss = mean(loss.(T_trains, T_sols))

        âˆ‚uâˆ‚z_loss = mean(loss.(u_trains_gradients, u_sols_gradients))
        âˆ‚vâˆ‚z_loss = mean(loss.(v_trains_gradients, v_sols_gradients))
        âˆ‚Tâˆ‚z_loss = mean(loss.(T_trains_gradients, T_sols_gradients))
        
        losses = (u=u_loss, v=v_loss, T=T_loss, âˆ‚uâˆ‚z=âˆ‚uâˆ‚z_loss, âˆ‚vâˆ‚z=âˆ‚vâˆ‚z_loss, âˆ‚Tâˆ‚z=âˆ‚Tâˆ‚z_loss)
        scaled_losses = apply_loss_scalings(losses, loss_scalings)

        return sum(scaled_losses), scaled_losses
    end

    @info "Setting up optimization objective"

    if train_gradient
        f_loss = OptimizationFunction(loss_gradient_mpp, GalacticOptim.AutoZygote())
    else
        f_loss = OptimizationFunction(loss_mpp, GalacticOptim.AutoZygote())
    end

    prob_loss = OptimizationProblem(f_loss, scaled_parameters, lb=[0f0, 0f0, 0f0, 0f0, 0f0], ub=[10f0, 10f0, 10f0, 10f0, 10f0])

    @info "Writing metadata"

    write_metadata_modified_pacanowski_philander_optimisation(FILE_PATH, train_files, maxiters, tsteps, unscale_parameter.(scaled_parameters, scalings.parameters), loss_scalings, optimizers)
    
    @info "Beginning Training"
    
    res = 0
    for i in 1:length(optimizers)
        iter = 1
        opt = optimizers[i]
        function cb(args...)
            if iter <= maxiters
                parameters = args[1]
                unscaled_parameters = unscale_parameter.(parameters, scalings.parameters)
                Î½â‚€, Î½â‚‹, Î”Ri, Riá¶œ, Pr = unscaled_parameters
                losses = args[3]
                profile_loss = losses.u + losses.v + losses.T
                gradient_loss = losses.âˆ‚uâˆ‚z + losses.âˆ‚vâˆ‚z + losses.âˆ‚Tâˆ‚z
                total_loss = profile_loss + gradient_loss
                @info "Î½â‚€ = $Î½â‚€, Î½â‚‹ = $Î½â‚‹, Î”Ri = $Î”Ri, Riá¶œ = $Riá¶œ, Pr = $Pr, loss = $(total_loss): profile = $profile_loss, gradient = $gradient_loss (u = $(losses.u), v = $(losses.v), T = $(losses.T)) , âˆ‚uâˆ‚z = $(losses.âˆ‚uâˆ‚z), âˆ‚vâˆ‚z = $(losses.âˆ‚vâˆ‚z), âˆ‚Tâˆ‚z = $(losses.âˆ‚Tâˆ‚z)), optimizer $i/$(length(optimizers)), iteration = $iter/$maxiters"
                write_data_modified_pacanowski_philander_optimisation(FILE_PATH, losses, unscaled_parameters)
            end
            iter += 1
            false
        end

        res = solve(prob_loss, opt, cb=cb, maxiters=maxiters)
        scaled_parameters .= res.minimizer
    end

    Î½â‚€, Î½â‚‹, Î”Ri, Riá¶œ = unscale_parameter.(scaled_parameters, scalings.parameters)
    @info "Î½â‚€ = $Î½â‚€, Î½â‚‹ = $Î½â‚‹, Î”Ri = $Î”Ri, Riá¶œ = $Riá¶œ, Pr = $Pr, loss = $loss"
end

function optimise_modified_pacanowski_philander_noÎ½â‚€(train_files, tsteps, timestepper, optimizers, maxiters, FILE_PATH;
                                                n_simulations, Î½â‚€ = 1f-5, Î½â‚‹ = 1f-1, Î”Ri=0.1f0, Riá¶œ=0.25f0, Pr=1f0,
                                                train_gradient=true, gradient_scaling=5f-3,
                                                training_fractions=nothing)
    @info "Loading training data"
    ð’Ÿ = WindMixing.data(train_files, scale_type=ZeroMeanUnitVarianceScaling, enforce_surface_fluxes=false)
    
    @info "Preparing constants and parameters"

    # Î½â‚€_scaling = 1 / Î½â‚€
    Î½â‚‹_scaling = 1 / Î½â‚‹
    Î”Ri_scaling = 1 / Î”Ri
    Riá¶œ_scaling = 1 / Riá¶œ
    Pr_scaling = 1 / Pr

    function scale_parameter(parameter, scaling)
        return parameter * scaling
    end

    function unscale_parameter(parameter, scaling)
        return parameter / scaling
    end
    
    function prepare_parameters()
        Nz = length(ð’Ÿ.u.z)
        H = abs(ð’Ÿ.uw.z[end] - ð’Ÿ.uw.z[1])
        Ï„ = abs(ð’Ÿ.t[:,1][end] - ð’Ÿ.t[:,1][1])
        u_scaling = ð’Ÿ.scalings["u"]
        v_scaling = ð’Ÿ.scalings["v"]
        T_scaling = ð’Ÿ.scalings["T"]
        uw_scaling = ð’Ÿ.scalings["uw"]
        vw_scaling = ð’Ÿ.scalings["vw"]
        wT_scaling = ð’Ÿ.scalings["wT"]

        constants = (H=H, Ï„=Ï„, Nz=Nz, f=1f-4, Î±=2f-4, g=9.80665f0)
        scalings = (u=u_scaling, v=v_scaling, T=T_scaling, uw=uw_scaling, vw=vw_scaling, wT=wT_scaling, parameters=[Î½â‚‹_scaling, Î”Ri_scaling, Riá¶œ_scaling, Pr_scaling])
        derivatives = (cell=Float32.(Dá¶œ(Nz, 1 / Nz)), face=Float32.(Dá¶ (Nz, 1 / Nz)))
        parameters = [Î½â‚‹, Î”Ri, Riá¶œ, Pr]
        scaled_parameters = scale_parameter.(parameters, scalings.parameters)

        return constants, scalings, derivatives, scaled_parameters
    end
    
    constants, scalings, derivatives, scaled_parameters = prepare_parameters()

    Nz = constants.Nz

    D_face = derivatives.face

    n_steps = Int(length(@view(ð’Ÿ.t[:,1])) / n_simulations)

    uvTâ‚€s = [ð’Ÿ.uvT_scaled[:,n_steps * i + tsteps[1]] for i in 0:n_simulations - 1]
    t_train = ð’Ÿ.t[:,1][tsteps] ./ constants.Ï„
    tspan_train = (t_train[1], t_train[end])
    uvT_trains = [ð’Ÿ.uvT_scaled[:,n_steps * i + 1:n_steps * (i + 1)][:, tsteps] for i in 0:n_simulations - 1]

    u_trains, v_trains, T_trains = split_u.(uvT_trains, Nz), split_v.(uvT_trains, Nz), split_T.(uvT_trains, Nz)

    if train_gradient
        u_trains_gradients = [âˆ‚_âˆ‚z(sol, D_face) for sol in u_trains]
        v_trains_gradients = [âˆ‚_âˆ‚z(sol, D_face) for sol in v_trains]
        T_trains_gradients = [âˆ‚_âˆ‚z(sol, D_face) for sol in T_trains]
    end    

    @info "Setting up BCs"

    @inline function BC(i)
        index = n_steps * i + tsteps[1]
        return (uw=(bottom=ð’Ÿ.uw.scaled[1,index], top=ð’Ÿ.uw.scaled[end, index]),
                vw=(bottom=ð’Ÿ.vw.scaled[1,index], top=ð’Ÿ.vw.scaled[end, index]),
                wT=(bottom=ð’Ÿ.wT.scaled[1,index], top=ð’Ÿ.wT.scaled[end, index]))
    end

    BCs = [BC(i) for i in 0:n_simulations - 1]    

    @info "Setting up differential equations"

    prob_NDEs = [ODEProblem((x, p, t) -> DE(x, p, t, derivatives, scalings, constants, BCs[i]), uvTâ‚€s[i], tspan_train) for i in 1:n_simulations]

    function determine_loss_scalings()
        if training_fractions === nothing
            loss_scalings = (u=1, v=1, T=1, âˆ‚uâˆ‚z=gradient_scaling, âˆ‚vâˆ‚z=gradient_scaling, âˆ‚Tâˆ‚z=gradient_scaling)
        else
            unscaled_parameters = unscale_parameter.(scaled_parameters, scalings.parameters)
            all_parameters = vcat(Î½â‚€, unscaled_parameters)

            sols = [Array(solve(prob_NDEs[i], timestepper, p=all_parameters, reltol=1f-3, sensealg=InterpolatingAdjoint(autojacvec=ZygoteVJP()), saveat=t_train)) for i in 1:n_simulations]        
            u_sols, v_sols, T_sols = split_u.(sols, Nz), split_v.(sols, Nz), split_T.(sols, Nz)

            u_loss = mean(loss.(u_trains, u_sols))
            v_loss = mean(loss.(v_trains, v_sols))
            T_loss = mean(loss.(T_trains, T_sols))

            if train_gradient
                u_sols_gradients = [âˆ‚_âˆ‚z(sol, D_face) for sol in u_sols]
                v_sols_gradients = [âˆ‚_âˆ‚z(sol, D_face) for sol in v_sols]
                T_sols_gradients = [âˆ‚_âˆ‚z(sol, D_face) for sol in T_sols]

                âˆ‚uâˆ‚z_loss = mean(loss.(u_trains_gradients, u_sols_gradients))
                âˆ‚vâˆ‚z_loss = mean(loss.(v_trains_gradients, v_sols_gradients))
                âˆ‚Tâˆ‚z_loss = mean(loss.(T_trains_gradients, T_sols_gradients))
            else
                âˆ‚uâˆ‚z_loss = 0
                âˆ‚vâˆ‚z_loss = 0
                âˆ‚Tâˆ‚z_loss = 0
            end

            losses = (u=u_loss, v=v_loss, T=T_loss, âˆ‚uâˆ‚z=âˆ‚uâˆ‚z_loss, âˆ‚vâˆ‚z=âˆ‚vâˆ‚z_loss, âˆ‚Tâˆ‚z=âˆ‚Tâˆ‚z_loss)
            loss_scalings = calculate_loss_scalings(losses, training_fractions, train_gradient)
        end
        return loss_scalings
    end

    @info "Determining training scalings"

    loss_scalings = determine_loss_scalings()

    function loss_mpp(parameters, p)
        unscaled_parameters = unscale_parameter.(parameters, scalings.parameters)
        all_parameters = vcat(Î½â‚€, unscaled_parameters)

        sols = [Array(solve(prob_NDEs[i], timestepper, p=all_parameters, reltol=1f-3, sensealg=InterpolatingAdjoint(autojacvec=ZygoteVJP()), saveat=t_train)) for i in 1:n_simulations]        
        
        u_sols, v_sols, T_sols = split_u.(sols, Nz), split_v.(sols, Nz), split_T.(sols, Nz)
        u_loss = mean(loss.(u_trains, u_sols))
        v_loss = mean(loss.(v_trains, v_sols))
        T_loss = mean(loss.(T_trains, T_sols))

        losses = (u=u_loss, v=v_loss, T=T_loss, âˆ‚uâˆ‚z=0, âˆ‚vâˆ‚z=0, âˆ‚Tâˆ‚z=0)
        scaled_losses = apply_loss_scalings(losses, loss_scalings)

        return sum(scaled_losses), scaled_losses
    end

    function loss_gradient_mpp(parameters, p)
        unscaled_parameters = unscale_parameter.(parameters, scalings.parameters)
        all_parameters = vcat(Î½â‚€, unscaled_parameters)

        sols = [Array(solve(prob_NDEs[i], timestepper, p=all_parameters, reltol=1f-3, sensealg=InterpolatingAdjoint(autojacvec=ZygoteVJP()), saveat=t_train)) for i in 1:n_simulations]        
        
        u_sols, v_sols, T_sols = split_u.(sols, Nz), split_v.(sols, Nz), split_T.(sols, Nz)

        u_sols_gradients = [âˆ‚_âˆ‚z(sol, D_face) for sol in u_sols]
        v_sols_gradients = [âˆ‚_âˆ‚z(sol, D_face) for sol in v_sols]
        T_sols_gradients = [âˆ‚_âˆ‚z(sol, D_face) for sol in T_sols]

        u_loss = mean(loss.(u_trains, u_sols))
        v_loss = mean(loss.(v_trains, v_sols))
        T_loss = mean(loss.(T_trains, T_sols))

        âˆ‚uâˆ‚z_loss = mean(loss.(u_trains_gradients, u_sols_gradients))
        âˆ‚vâˆ‚z_loss = mean(loss.(v_trains_gradients, v_sols_gradients))
        âˆ‚Tâˆ‚z_loss = mean(loss.(T_trains_gradients, T_sols_gradients))
        
        losses = (u=u_loss, v=v_loss, T=T_loss, âˆ‚uâˆ‚z=âˆ‚uâˆ‚z_loss, âˆ‚vâˆ‚z=âˆ‚vâˆ‚z_loss, âˆ‚Tâˆ‚z=âˆ‚Tâˆ‚z_loss)
        scaled_losses = apply_loss_scalings(losses, loss_scalings)

        return sum(scaled_losses), scaled_losses
    end

    @info "Setting up optimization objective"

    if train_gradient
        f_loss = OptimizationFunction(loss_gradient_mpp, GalacticOptim.AutoZygote())
    else
        f_loss = OptimizationFunction(loss_mpp, GalacticOptim.AutoZygote())
    end

    prob_loss = OptimizationProblem(f_loss, scaled_parameters, lb=[0f0, 0f0, 0f0, 0f0], ub=[10f0, 10f0, 10f0, 10f0])

    @info "Writing metadata"

    write_metadata_modified_pacanowski_philander_optimisation(FILE_PATH, train_files, maxiters, tsteps, unscale_parameter.(scaled_parameters, scalings.parameters), loss_scalings, optimizers)
    
    @info "Beginning Training"
    
    res = 0
    for i in 1:length(optimizers)
        iter = 1
        opt = optimizers[i]
        function cb(args...)
            if iter <= maxiters
                parameters = args[1]
                unscaled_parameters = unscale_parameter.(parameters, scalings.parameters)
                # Î½â‚€, Î½â‚‹, Î”Ri, Riá¶œ, Pr = unscaled_parameters
                Î½â‚‹, Î”Ri, Riá¶œ, Pr = unscaled_parameters

                losses = args[3]
                profile_loss = losses.u + losses.v + losses.T
                gradient_loss = losses.âˆ‚uâˆ‚z + losses.âˆ‚vâˆ‚z + losses.âˆ‚Tâˆ‚z
                total_loss = profile_loss + gradient_loss
                @info "Î½â‚€ = $Î½â‚€, Î½â‚‹ = $Î½â‚‹, Î”Ri = $Î”Ri, Riá¶œ = $Riá¶œ, Pr = $Pr, loss = $(total_loss): profile = $profile_loss, gradient = $gradient_loss (u = $(losses.u), v = $(losses.v), T = $(losses.T)) , âˆ‚uâˆ‚z = $(losses.âˆ‚uâˆ‚z), âˆ‚vâˆ‚z = $(losses.âˆ‚vâˆ‚z), âˆ‚Tâˆ‚z = $(losses.âˆ‚Tâˆ‚z)), optimizer $i/$(length(optimizers)), iteration = $iter/$maxiters"
                write_data_modified_pacanowski_philander_optimisation(FILE_PATH, losses, unscaled_parameters)
            end
            iter += 1
            false
        end

        res = solve(prob_loss, opt, cb=cb, maxiters=maxiters)
        scaled_parameters .= res.minimizer
    end

    # Î½â‚€, Î½â‚‹, Î”Ri, Riá¶œ = unscale_parameter.(scaled_parameters, scalings.parameters)
    Î½â‚‹, Î”Ri, Riá¶œ, Pr = unscale_parameter.(scaled_parameters, scalings.parameters)

    @info "Î½â‚€ = $Î½â‚€, Î½â‚‹ = $Î½â‚‹, Î”Ri = $Î”Ri, Riá¶œ = $Riá¶œ, Pr = $Pr, loss = $loss"
end

function optimise_modified_pacanowski_philander_noÎ½â‚€Pr(train_files, tsteps, timestepper, optimizers, maxiters, FILE_PATH;
                                                n_simulations, Î½â‚€ = 1f-5, Î½â‚‹ = 1f-1, Î”Ri=0.1f0, Riá¶œ=0.25f0, Pr=1f0,
                                                train_gradient=true, gradient_scaling=5f-3,
                                                training_fractions=nothing)
    @info "Loading training data"
    ð’Ÿ = WindMixing.data(train_files, scale_type=ZeroMeanUnitVarianceScaling, enforce_surface_fluxes=false)
    
    @info "Preparing constants and parameters"

    # Î½â‚€_scaling = 1 / Î½â‚€
    # Pr_scaling = 1 / Pr

    Î½â‚‹_scaling = 1 / Î½â‚‹
    Î”Ri_scaling = 1 / Î”Ri
    Riá¶œ_scaling = 1 / Riá¶œ

    function scale_parameter(parameter, scaling)
        return parameter * scaling
    end

    function unscale_parameter(parameter, scaling)
        return parameter / scaling
    end
    
    function prepare_parameters()
        Nz = length(ð’Ÿ.u.z)
        H = abs(ð’Ÿ.uw.z[end] - ð’Ÿ.uw.z[1])
        Ï„ = abs(ð’Ÿ.t[:,1][end] - ð’Ÿ.t[:,1][1])
        u_scaling = ð’Ÿ.scalings["u"]
        v_scaling = ð’Ÿ.scalings["v"]
        T_scaling = ð’Ÿ.scalings["T"]
        uw_scaling = ð’Ÿ.scalings["uw"]
        vw_scaling = ð’Ÿ.scalings["vw"]
        wT_scaling = ð’Ÿ.scalings["wT"]

        constants = (H=H, Ï„=Ï„, Nz=Nz, f=1f-4, Î±=2f-4, g=9.80665f0)
        scalings = (u=u_scaling, v=v_scaling, T=T_scaling, uw=uw_scaling, vw=vw_scaling, wT=wT_scaling, parameters=[Î½â‚‹_scaling, Î”Ri_scaling, Riá¶œ_scaling])
        derivatives = (cell=Float32.(Dá¶œ(Nz, 1 / Nz)), face=Float32.(Dá¶ (Nz, 1 / Nz)))
        parameters = [Î½â‚‹, Î”Ri, Riá¶œ]
        scaled_parameters = scale_parameter.(parameters, scalings.parameters)

        return constants, scalings, derivatives, scaled_parameters
    end
    
    constants, scalings, derivatives, scaled_parameters = prepare_parameters()

    Nz = constants.Nz

    D_face = derivatives.face

    n_steps = Int(length(@view(ð’Ÿ.t[:,1])) / n_simulations)

    uvTâ‚€s = [ð’Ÿ.uvT_scaled[:,n_steps * i + tsteps[1]] for i in 0:n_simulations - 1]
    t_train = ð’Ÿ.t[:,1][tsteps] ./ constants.Ï„
    tspan_train = (t_train[1], t_train[end])
    uvT_trains = [ð’Ÿ.uvT_scaled[:,n_steps * i + 1:n_steps * (i + 1)][:, tsteps] for i in 0:n_simulations - 1]

    u_trains, v_trains, T_trains = split_u.(uvT_trains, Nz), split_v.(uvT_trains, Nz), split_T.(uvT_trains, Nz)

    if train_gradient
        u_trains_gradients = [âˆ‚_âˆ‚z(sol, D_face) for sol in u_trains]
        v_trains_gradients = [âˆ‚_âˆ‚z(sol, D_face) for sol in v_trains]
        T_trains_gradients = [âˆ‚_âˆ‚z(sol, D_face) for sol in T_trains]
    end    

    @info "Setting up BCs"

    @inline function BC(i)
        index = n_steps * i + tsteps[1]
        return (uw=(bottom=ð’Ÿ.uw.scaled[1,index], top=ð’Ÿ.uw.scaled[end, index]),
                vw=(bottom=ð’Ÿ.vw.scaled[1,index], top=ð’Ÿ.vw.scaled[end, index]),
                wT=(bottom=ð’Ÿ.wT.scaled[1,index], top=ð’Ÿ.wT.scaled[end, index]))
    end

    BCs = [BC(i) for i in 0:n_simulations - 1]    

    @info "Setting up differential equations"

    prob_NDEs = [ODEProblem((x, p, t) -> DE(x, p, t, derivatives, scalings, constants, BCs[i]), uvTâ‚€s[i], tspan_train) for i in 1:n_simulations]

    function determine_loss_scalings()
        if training_fractions === nothing
            loss_scalings = (u=1, v=1, T=1, âˆ‚uâˆ‚z=gradient_scaling, âˆ‚vâˆ‚z=gradient_scaling, âˆ‚Tâˆ‚z=gradient_scaling)
        else
            unscaled_parameters = unscale_parameter.(scaled_parameters, scalings.parameters)
            all_parameters = vcat(Î½â‚€, unscaled_parameters, Pr)

            sols = [Array(solve(prob_NDEs[i], timestepper, p=all_parameters, reltol=1f-3, sensealg=InterpolatingAdjoint(autojacvec=ZygoteVJP()), saveat=t_train)) for i in 1:n_simulations]        
            u_sols, v_sols, T_sols = split_u.(sols, Nz), split_v.(sols, Nz), split_T.(sols, Nz)

            u_loss = mean(loss.(u_trains, u_sols))
            v_loss = mean(loss.(v_trains, v_sols))
            T_loss = mean(loss.(T_trains, T_sols))

            if train_gradient
                u_sols_gradients = [âˆ‚_âˆ‚z(sol, D_face) for sol in u_sols]
                v_sols_gradients = [âˆ‚_âˆ‚z(sol, D_face) for sol in v_sols]
                T_sols_gradients = [âˆ‚_âˆ‚z(sol, D_face) for sol in T_sols]

                âˆ‚uâˆ‚z_loss = mean(loss.(u_trains_gradients, u_sols_gradients))
                âˆ‚vâˆ‚z_loss = mean(loss.(v_trains_gradients, v_sols_gradients))
                âˆ‚Tâˆ‚z_loss = mean(loss.(T_trains_gradients, T_sols_gradients))
            else
                âˆ‚uâˆ‚z_loss = 0
                âˆ‚vâˆ‚z_loss = 0
                âˆ‚Tâˆ‚z_loss = 0
            end

            losses = (u=u_loss, v=v_loss, T=T_loss, âˆ‚uâˆ‚z=âˆ‚uâˆ‚z_loss, âˆ‚vâˆ‚z=âˆ‚vâˆ‚z_loss, âˆ‚Tâˆ‚z=âˆ‚Tâˆ‚z_loss)
            loss_scalings = calculate_loss_scalings(losses, training_fractions, train_gradient)
        end
        return loss_scalings
    end

    @info "Determining training scalings"

    loss_scalings = determine_loss_scalings()

    function loss_mpp(parameters, p)
        unscaled_parameters = unscale_parameter.(parameters, scalings.parameters)
        all_parameters = vcat(Î½â‚€, unscaled_parameters, Pr)

        sols = [Array(solve(prob_NDEs[i], timestepper, p=all_parameters, reltol=1f-3, sensealg=InterpolatingAdjoint(autojacvec=ZygoteVJP()), saveat=t_train)) for i in 1:n_simulations]        
        
        u_sols, v_sols, T_sols = split_u.(sols, Nz), split_v.(sols, Nz), split_T.(sols, Nz)
        u_loss = mean(loss.(u_trains, u_sols))
        v_loss = mean(loss.(v_trains, v_sols))
        T_loss = mean(loss.(T_trains, T_sols))

        losses = (u=u_loss, v=v_loss, T=T_loss, âˆ‚uâˆ‚z=0, âˆ‚vâˆ‚z=0, âˆ‚Tâˆ‚z=0)
        scaled_losses = apply_loss_scalings(losses, loss_scalings)

        return sum(scaled_losses), scaled_losses
    end

    function loss_gradient_mpp(parameters, p)
        unscaled_parameters = unscale_parameter.(parameters, scalings.parameters)
        all_parameters = vcat(Î½â‚€, unscaled_parameters)

        sols = [Array(solve(prob_NDEs[i], timestepper, p=all_parameters, reltol=1f-3, sensealg=InterpolatingAdjoint(autojacvec=ZygoteVJP()), saveat=t_train)) for i in 1:n_simulations]        
        
        u_sols, v_sols, T_sols = split_u.(sols, Nz), split_v.(sols, Nz), split_T.(sols, Nz)

        u_sols_gradients = [âˆ‚_âˆ‚z(sol, D_face) for sol in u_sols]
        v_sols_gradients = [âˆ‚_âˆ‚z(sol, D_face) for sol in v_sols]
        T_sols_gradients = [âˆ‚_âˆ‚z(sol, D_face) for sol in T_sols]

        u_loss = mean(loss.(u_trains, u_sols))
        v_loss = mean(loss.(v_trains, v_sols))
        T_loss = mean(loss.(T_trains, T_sols))

        âˆ‚uâˆ‚z_loss = mean(loss.(u_trains_gradients, u_sols_gradients))
        âˆ‚vâˆ‚z_loss = mean(loss.(v_trains_gradients, v_sols_gradients))
        âˆ‚Tâˆ‚z_loss = mean(loss.(T_trains_gradients, T_sols_gradients))
        
        losses = (u=u_loss, v=v_loss, T=T_loss, âˆ‚uâˆ‚z=âˆ‚uâˆ‚z_loss, âˆ‚vâˆ‚z=âˆ‚vâˆ‚z_loss, âˆ‚Tâˆ‚z=âˆ‚Tâˆ‚z_loss)
        scaled_losses = apply_loss_scalings(losses, loss_scalings)

        return sum(scaled_losses), scaled_losses
    end

    @info "Setting up optimization objective"

    if train_gradient
        f_loss = OptimizationFunction(loss_gradient_mpp, GalacticOptim.AutoZygote())
    else
        f_loss = OptimizationFunction(loss_mpp, GalacticOptim.AutoZygote())
    end

    prob_loss = OptimizationProblem(f_loss, scaled_parameters, lb=[0f0, 0f0, 0f0], ub=[10f0, 10f0, 10f0])

    @info "Writing metadata"

    write_metadata_modified_pacanowski_philander_optimisation(FILE_PATH, train_files, maxiters, tsteps, unscale_parameter.(scaled_parameters, scalings.parameters), loss_scalings, optimizers)
    
    @info "Beginning Training"
    
    res = 0
    for i in 1:length(optimizers)
        iter = 1
        opt = optimizers[i]
        function cb(args...)
            if iter <= maxiters
                parameters = args[1]
                unscaled_parameters = unscale_parameter.(parameters, scalings.parameters)
                # Î½â‚€, Î½â‚‹, Î”Ri, Riá¶œ, Pr = unscaled_parameters
                Î½â‚‹, Î”Ri, Riá¶œ = unscaled_parameters

                losses = args[3]
                profile_loss = losses.u + losses.v + losses.T
                gradient_loss = losses.âˆ‚uâˆ‚z + losses.âˆ‚vâˆ‚z + losses.âˆ‚Tâˆ‚z
                total_loss = profile_loss + gradient_loss
                @info "Î½â‚€ = $Î½â‚€, Î½â‚‹ = $Î½â‚‹, Î”Ri = $Î”Ri, Riá¶œ = $Riá¶œ, Pr = $Pr, loss = $(total_loss): profile = $profile_loss, gradient = $gradient_loss (u = $(losses.u), v = $(losses.v), T = $(losses.T)) , âˆ‚uâˆ‚z = $(losses.âˆ‚uâˆ‚z), âˆ‚vâˆ‚z = $(losses.âˆ‚vâˆ‚z), âˆ‚Tâˆ‚z = $(losses.âˆ‚Tâˆ‚z)), optimizer $i/$(length(optimizers)), iteration = $iter/$maxiters"
                write_data_modified_pacanowski_philander_optimisation(FILE_PATH, losses, unscaled_parameters)
            end
            iter += 1
            false
        end

        res = solve(prob_loss, opt, cb=cb, maxiters=maxiters)
        scaled_parameters .= res.minimizer
    end

    Î½â‚‹, Î”Ri, Riá¶œ = unscale_parameter.(scaled_parameters, scalings.parameters)

    @info "Î½â‚€ = $Î½â‚€, Î½â‚‹ = $Î½â‚‹, Î”Ri = $Î”Ri, Riá¶œ = $Riá¶œ, Pr = $Pr, loss = $loss"
end